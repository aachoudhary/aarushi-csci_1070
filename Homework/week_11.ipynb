{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo       object\n",
       "StockCode       object\n",
       "Description     object\n",
       "Quantity         int64\n",
       "InvoiceDate     object\n",
       "UnitPrice      float64\n",
       "CustomerID     float64\n",
       "Country         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "df=pd.read_csv('data.csv',encoding='unicode_escape')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Preprocess the data. How are you handling nulls? What process(es) are you using to encode and normalize the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.dropna(inplace=True)\n",
    "#I handled the nulls by removing the rows with missing data\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%m/%d/%Y %H:%M')\n",
    "df[\"Description\"] = df[\"Description\"].str.lower()\n",
    "\n",
    "\n",
    "df = df[df['Quantity'] > 0]\n",
    "\n",
    "df.insert(loc=2, column='year_month', value=df['InvoiceDate'].map(lambda x: 100*x.year + x.month))\n",
    "\n",
    "df.insert(loc=3, column='month', value=df.InvoiceDate.dt.month)\n",
    "\n",
    "\n",
    "df.insert(loc=4, column='day', value=(df.InvoiceDate.dt.dayofweek)+1)\n",
    "\n",
    "df.insert(loc=5, column='hour', value=df.InvoiceDate.dt.hour)\n",
    "\n",
    "df[\"amount_spent\"] = df[\"UnitPrice\"]*df[\"Quantity\"]\n",
    "\n",
    "\n",
    "\n",
    "#Here I made the data easier to read by changing formatting and also adding a column to help understand the \n",
    "#data better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform k-means on this dataset for customer segments. Customer segments help determine what types of people buy your product, which allows you to target more people like your usual customers. Should you look at all the data, or which subset of data should you use? What is the ideal number of clusters? Which approach did you use to find the ideal number of clusters and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.27425619e-04 -5.62369214e-04 -9.59928532e-01]\n",
      " [ 5.45039220e+02  4.48852598e+02  1.59206719e+00]\n",
      " [ 2.49668037e+02  4.11273612e+02 -1.46390243e+00]\n",
      " [-4.24814054e-03 -3.48275916e-03  7.93077706e-01]]\n"
     ]
    }
   ],
   "source": [
    "inertia = []\n",
    "customer_features = ['amount_spent', 'Quantity', 'InvoiceNo']\n",
    "X = df[customer_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "df['clusters'] = kmeans.labels_\n",
    "\n",
    "def elbow_method(X, k):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "  kmeans.fit(X)\n",
    "  return kmeans.inertia_\n",
    "\n",
    "print(kmeans.cluster_centers_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should not use the entire dataset, but the sections that show frequency and amount spent. This way we know how much each customer bought and how much they spent, which allowed us to figure out what is more popular. I used the elbow method as it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform hierarchical clustering for customer segments. What is the ideal number of clusters? Which approach did you use to find the ideal number of clusters and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=3,linkage='ward')\n",
    "cluster.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Compare the results of 2 and 3. Which approach do you think is best? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that elbow/k-means was better because we have such a large set of data, so its better suited for it. Additionally, we don't need to see the relationship between each number spent, but more so the person, which is why k-means is better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
